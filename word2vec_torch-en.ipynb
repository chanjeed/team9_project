{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 必要であれば変更してください\\nimport os\\nos.chdir(\\'/root/userspace/team9_project\\')\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "# 必要であれば変更してください\n",
    "import os\n",
    "os.chdir('/root/userspace/team9_project')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"『こころ』を読み込むための関数\n",
    "\n",
    "    :param path: str, 『こころ』のパス\n",
    "    :return text: list of list of str, 各文がトークナイズされた『こころ』\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = nltk.word_tokenize(line)\n",
    "            text.append(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'i', 'am', 'mako', '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = \"hello, i am mako.\"\n",
    "nltk.word_tokenize(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data(\"dataset/Material/recipe1M/recipe1M_0.txt\")\n",
    "#text = load_data(\"dataset/Material/recipes2.txt\")\n",
    "#text = load_data(\"dataset/kokoro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊なトークンとそのIDは事前に定義しておきます。\n",
    "PAD_TOKEN = '<PAD>' # あとで説明するpaddingに使います\n",
    "UNK_TOKEN = '<UNK>' # 辞書にない単語は全てこのUNKトークンに置き換えます。(UNKの由来はunkownです)\n",
    "PAD = 0 # <PAD>のID\n",
    "UNK = 1 # <UNK>のID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書の初期化\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "# 辞書に含める単語の最低出現回数\n",
    "# 今回はコーパスのサイズが小さいので、全ての単語を辞書に含めることにします\n",
    "MIN_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"語彙を管理するためのクラス\"\"\"\n",
    "    def __init__(self, word2id={}):\n",
    "        \"\"\"\n",
    "        :param word2id: 単語(str)をインデックス(int)に変換する辞書\n",
    "        \"\"\"\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
    "\n",
    "    def build_vocab(self, sentences, min_count=1):\n",
    "        \"\"\"コーパスから語彙の辞書を構築するメソッド\n",
    "\n",
    "        :param sentences: list of list of str, コーパス\n",
    "        :param min_count: int, 辞書に含める単語の最小出現回数\n",
    "        \"\"\"\n",
    "        # 各単語の出現回数をカウントする辞書を作成します\n",
    "        word_counter = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                # dict.get(key, 0)はdictにkeyがあればdict[key]を、なければ0を返すメソッドです\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "        # min_count回以上出現する単語のみ語彙に加えます\n",
    "        # 出現回数の高い単語から順にword2idに追加していきます\n",
    "        # 出現回数に-1をかけた値でsortすることで出現回数の降順になるようにしています\n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "\n",
    "        # 語彙に含まれる単語の出現回数を保持します（あとで使います）\n",
    "        self.raw_vocab = {w: word_counter[w] for w in self.word2id.keys() if w in word_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙数: 23245\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(text, min_count=MIN_COUNT)\n",
    "print(\"語彙数:\", len(vocab.word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sen):\n",
    "    \"\"\"\n",
    "    単語のリストをIDのリストに変換する関数\n",
    "\n",
    "    :param vocab: class `Vocab` object\n",
    "    :param sen: list of str, 文を分かち書きして得られた単語のリスト\n",
    "    :return out: list of int, 単語IDのリスト\n",
    "    \"\"\"\n",
    "    out = [vocab.word2id.get(word, UNK) for word in sen] # 辞書にない単語にはUNKのIDを割り振ります\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語のテキストを単語IDに変換します。\n",
    "id_text = [sentence_to_ids(vocab, sen) for sen in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(id_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    \"\"\"Paddingを行う関数\n",
    "\n",
    "    :param seq: list of int, 単語のインデックスのリスト\n",
    "    :param max_length: int, バッチ内の系列の最大長\n",
    "    :return seq: list of int, 単語のインデックスのリスト\n",
    "    \"\"\"\n",
    "    seq += [PAD for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 64 # ミニバッチのサイズ\n",
    "n_batches = 1000 # 今回学習するミニバッチの数\n",
    "vocab_size = len(vocab.word2id) # 語彙の総数\n",
    "embedding_size = 500 # 各単語に割り当てるベクトルの次元数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestIter(object):\n",
    "    def __init__(self):\n",
    "        self.iter = 0\n",
    "        self.max_iter = 5\n",
    "    \n",
    "    def __iter__(self): # 必須\n",
    "        print(\"iter関数が呼び出されました\")\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.iter += 1\n",
    "        print(\"next関数が呼び出されました({}回目)\".format(self.iter))\n",
    "        if self.iter < self.max_iter:\n",
    "            return None\n",
    "        else:\n",
    "            print(\"max_iterに達したので終了します\")\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderCBOW(object):\n",
    "    \"\"\"CBOWのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語とターゲットの単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            # 走査する対象の文\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # 予測すべき単語\n",
    "            word_Y = sen[self.w_pointer]\n",
    "            \n",
    "            # 入力となる単語群を取得\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_X = pad_seq(word_X, self.window * 2)\n",
    "            \n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "            \n",
    "            if self.w_pointer >= len(sen):\n",
    "                # 文を走査し終わったら次の文の先頭にポインタを移行する\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    # 全ての文を走査し終わったら終了する\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        # データはtorch.Tensorにする必要があります。dtype, deviceも指定します。\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 全結合層(バイアスなし)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), CBOWのloss\n",
    "        \"\"\"\n",
    "        emb_X = self.embedding(batch_X) # (batch_size, window*2, embedding_size)\n",
    "        # paddingした部分を無視するためにマスクをかけます\n",
    "        emb_X = emb_X * (batch_X != PAD).float().unsqueeze(-1) # (batch_size, window*2, embedding_size)\n",
    "        sum_X = torch.sum(emb_X, dim=1) # (batch_size, embedding_size)\n",
    "        lin_X = self.linear(sum_X) # (batch_size, vocab_size)\n",
    "        log_prob_X = F.log_softmax(lin_X, dim=-1) # (batch_size, vocab_size)\n",
    "        loss = F.nll_loss(log_prob_X, batch_Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル\n",
    "cbow = CBOW(vocab_size, embedding_size).to(device) # iLectで実行する場合warning (GPU is too old) が出ますが, 実行に問題はないので気にせず進めてください.\n",
    "# optimizer\n",
    "optimizer_cbow = optim.Adam(cbow.parameters())\n",
    "# データローダー\n",
    "dataloader_cbow = DataLoaderCBOW(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs, optimizer=None, is_train=True):\n",
    "    \"\"\"lossを計算するための関数\n",
    "    \n",
    "    is_train=Trueならモデルをtrainモードに、\n",
    "    is_train=Falseならモデルをevaluationモードに設定します\n",
    "    \n",
    "    :param model: 学習させるモデル\n",
    "    :param inputs: モデルへの入力\n",
    "    :param optimizer: optimizer\n",
    "    :param is_train: bool, モデルtrainさせるか否か\n",
    "    \"\"\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    # lossを計算します。\n",
    "    loss = model(*inputs)\n",
    "\n",
    "    if is_train:\n",
    "        # .backward()を実行する前にmodelのparameterのgradientを全て0にセットします\n",
    "        optimizer.zero_grad()\n",
    "        # parameterのgradientを計算します。\n",
    "        loss.backward()\n",
    "        # parameterのgradientを用いてparameterを更新します。\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:10.0546\n",
      "batch:100, loss:7.1398\n",
      "batch:200, loss:6.0379\n",
      "batch:300, loss:6.4666\n",
      "batch:400, loss:5.4934\n",
      "batch:500, loss:6.1790\n",
      "batch:600, loss:6.2294\n",
      "batch:700, loss:7.2893\n",
      "batch:800, loss:6.1183\n",
      "batch:900, loss:5.6929\n",
      "batch:1000, loss:5.6241\n",
      "Elapsed time: 19.09 [sec]\n"
     ]
    }
   ],
   "source": [
    "start_at = time.time()\n",
    "\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_cbow):\n",
    "    loss = compute_loss(cbow, (batch_X, batch_Y), optimizer=optimizer_cbow, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "\n",
    "end_at = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"dataset/recipe_en/recipe_en_cbow_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"recipe_cbow_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSG(object):\n",
    "    \"\"\"Skipgramのためのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # Skipgramでは入力が1単語\n",
    "            word_X = sen[self.w_pointer]\n",
    "\n",
    "            # 出力は周辺単語\n",
    "            start = max(0,self.w_pointer-self.window)\n",
    "            word_Y = sen[start:self.w_pointer]+sen[self.w_pointer+1:self.w_pointer+self.window+1]\n",
    "            word_Y = pad_seq(word_Y,self.window*2)\n",
    "\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), Skipgramのloss\n",
    "        \"\"\"\n",
    "        emb_X = self.embedding(batch_X)\n",
    "        lin_X = self.linear(emb_X)\n",
    "        log_prob_X = F.log_softmax(lin_X,dim=-1)\n",
    "        log_prob_X = torch.gather(log_prob_X, 1, batch_Y) # (batch_size, window*2)\n",
    "        # paddingした単語のlossは計算しないようにマスクをかけます(=lossの該当部分を0にします)\n",
    "        log_prob_X = log_prob_X * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss = log_prob_X.sum(1).mean().neg()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Skipgram(vocab_size, embedding_size).to(device)\n",
    "optimizer_sg = optim.Adam(sg.parameters())\n",
    "dataloader_sg = DataLoaderSG(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:60.2855\n",
      "batch:100, loss:52.3704\n",
      "batch:200, loss:43.8030\n",
      "batch:300, loss:43.6989\n",
      "batch:400, loss:38.9968\n",
      "batch:500, loss:40.8202\n",
      "batch:600, loss:40.3956\n",
      "batch:700, loss:46.6930\n",
      "batch:800, loss:41.4886\n",
      "batch:900, loss:40.2747\n",
      "batch:1000, loss:38.2908\n",
      "Elapsed time: 18.94 [sec]\n"
     ]
    }
   ],
   "source": [
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_sg):\n",
    "    loss = compute_loss(sg, (batch_X, batch_Y), optimizer=optimizer_sg, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(sg.embedding.weight.data.cpu().numpy(),  \"dataset/recipe_en/recipe_en_sg_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(sg.embedding.weight.data.cpu().numpy(),  \"recipe_sg_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skipgram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samplingに使う確率分布\n",
    "weights = np.power([0, 0] + list(vocab.raw_vocab.values()), 0.75)\n",
    "weights = weights / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSGNS(object):\n",
    "    def __init__(self, text, batch_size, window=3, n_negative=5, weights=None):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        :param n_negative: int, 負例の数\n",
    "        :param weights: numpy.ndarray, Negative Samplingで使う確率分布\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.n_negative = n_negative\n",
    "        self.weights = None\n",
    "        if weights is not None:\n",
    "            self.weights = torch.FloatTensor(weights) # negative samplingに使う確率分布\n",
    "        self.s_pointer = 0 # 文のポインタ\n",
    "        self.w_pointer = 0 # 単語のポインタ\n",
    "        self.max_s_pointer = len(text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        batch_N = [] # 負例\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[self.w_pointer]\n",
    "            word_Y = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_Y = pad_seq(word_Y, self.window * 2)\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "\n",
    "            # 多項分布で負例をサンプリング\n",
    "            # 実装を簡略化するために、正例の除去は行っていません\n",
    "            negative_samples = torch.multinomial(self.weights, self.n_negative) # (n_negative,)\n",
    "            batch_N.append(negative_samples.unsqueeze(0)) # (1, n_negative)\n",
    "\n",
    "            self.w_pointer += 1\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        batch_N = torch.cat(batch_N, dim=0).to(device) # (batch_size, n_negative)\n",
    "\n",
    "        return batch_X, batch_Y, batch_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 入力単語の埋め込み層\n",
    "        self.i_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 出力単語の埋め込み層\n",
    "        self.o_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.i_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.o_embedding.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y, batch_N):\n",
    "        \"\"\"\n",
    "        :param batch_x: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_n: torch.Tensor(dtype=torch.long), (batch_size, n_negative)\n",
    "        \"\"\"\n",
    "        embed_X = self.i_embedding(batch_X).unsqueeze(2) # (batch_size, embedding_size, 1)\n",
    "        embed_Y = self.o_embedding(batch_Y) # (batch_size, window*2, embedding_size)\n",
    "        embed_N = self.o_embedding(batch_N).neg() # (batch_size, n_negative, embedding_size)\n",
    "        loss_Y = torch.bmm(embed_Y, embed_X).squeeze().sigmoid().log() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y.sum(1) # (batch_size,)\n",
    "        loss_N = torch.bmm(embed_N, embed_X).squeeze().sigmoid().log().sum(1) # (batch_size,)\n",
    "        return -(loss_Y + loss_N).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:7.5594\n",
      "batch:100, loss:7.3871\n",
      "batch:200, loss:7.0376\n",
      "batch:300, loss:7.0238\n",
      "batch:400, loss:6.4959\n",
      "batch:500, loss:6.8672\n",
      "batch:600, loss:6.8358\n",
      "batch:700, loss:7.1804\n",
      "batch:800, loss:6.8428\n",
      "batch:900, loss:6.7010\n",
      "batch:1000, loss:6.7130\n",
      "Elapsed time: 120.07 [sec]\n"
     ]
    }
   ],
   "source": [
    "sgns = SGNS(vocab_size, embedding_size).to(device)\n",
    "optimizer_sgns = optim.Adam(sgns.parameters())\n",
    "dataloader_sgns = DataLoaderSGNS(id_text, batch_size, n_negative=5, weights=weights)\n",
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y, batch_N) in enumerate(dataloader_sgns):\n",
    "    loss = compute_loss(sgns, (batch_X, batch_Y, batch_N), optimizer=optimizer_sgns, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddingのパラメータのみを保存する\n",
    "torch.save(sgns.i_embedding.weight.data.cpu().numpy(),  \"dataset/recipe_en/recipe_en_sgns_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(sgns.i_embedding.weight.data.cpu().numpy(),  \"recipe_sgns_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_similarity(embedding_path, word, n):\n",
    "    \"\"\"\n",
    "    与えられた単語に最も似ている単語とcos類似度を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "    embedding = torch.load(embedding_path)\n",
    "\n",
    "    # 単語ベクトルを全て単位ベクトルにする\n",
    "    norm = np.linalg.norm(embedding, ord=2, axis=1, keepdims=True)\n",
    "    norm = np.where(norm==0, 1, norm) # 0で割ることを避ける\n",
    "    embedding /= norm\n",
    "    e = embedding[vocab.word2id[word]]\n",
    "\n",
    "    # 単語ベクトル同士のcos類似度を計算する\n",
    "    cos_sim = np.dot(embedding, e.reshape(-1, 1)).reshape(-1,)\n",
    "    most_sim = np.argsort(cos_sim)[::-1][1:n+1] # 自分は除く\n",
    "    most_sim_words = [vocab.id2word[_id] for _id in most_sim]\n",
    "    top_cos_sim = cos_sim[most_sim]\n",
    "    out = \", \".join([w+\"({:.4f})\".format(v) for w, v in zip(most_sim_words, top_cos_sim)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow\t: black(0.7793), chili(0.7757), basil(0.7673), cumin(0.7595), vinegar(0.7373)\n",
      "sg\t: minute.Heat(0.1654), middle.Line(0.1653), walnuts.Spoon(0.1623), using.Stir(0.1603), halibut(0.1533)\n",
      "sgns\t: pepper(0.8780), ginger(0.8599), black(0.8588), lemon(0.8557), cumin(0.8518)\n"
     ]
    }
   ],
   "source": [
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"dataset/recipe_en/recipe_en_\"+model + \"_embedding_1000b_min_10_dim_500.pth\", \"salt\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow\t: squash(0.8741), thyme(0.8621), beans(0.8535), shallots(0.8437), ginger(0.8331)\n",
      "sg\t: peeler.Cut(0.1689), choice(0.1662), instead(0.1637), terrific(0.1594), serving.Slice(0.1538)\n",
      "sgns\t: rosemary(0.8602), spinach(0.8585), celery(0.8512), apple(0.8495), cilantro(0.8477)\n"
     ]
    }
   ],
   "source": [
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"dataset/recipe_en/recipe_en_\"+model + \"_embedding_1000b_min_10_dim_500.pth\", \"mushrooms\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(e1, e2):\n",
    "    \"\"\"\n",
    "    与えられた単語に最も似ている単語とcos類似度を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "\n",
    "    norm2 = np.linalg.norm(e2, ord=2)\n",
    "    norm2 = np.where(norm2==0, 1, norm2) # 0で割ることを避ける\n",
    "    e2 /= norm2\n",
    "    \n",
    "    norm1 = np.linalg.norm(e1, ord=2)\n",
    "    norm1 = np.where(norm1==0, 1, norm1) # 0で割ることを避ける\n",
    "    e1 /= norm1\n",
    "    \n",
    "    # 単語ベクトル同士のcos類似度を計算する\n",
    "    cos_sim = np.dot(e1, e2)\n",
    " \n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23245, 500)\n"
     ]
    }
   ],
   "source": [
    "embedding_path = \"dataset/recipe_en/recipe_en_cbow_embedding_1000b_min_10_dim_500.pth\"\n",
    "embedding = torch.load(embedding_path)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = embedding[vocab.word2id['salt']]\n",
    "e2 = embedding[vocab.word2id['pepper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7081826"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(e1,e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterVec = [embedding[0]]     # tracks sum of vectors in a cluster\n",
    "clusterIdx = []    # array of index arrays. e.g. [[1, 3, 5], [2, 4, 6]]\n",
    "ncluster = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23245\n"
     ]
    }
   ],
   "source": [
    "# probablity to create a new table if new customer\n",
    "# is not strongly \"similar\" to any existing table\n",
    "pnew = 1.0/ (1 + ncluster)  \n",
    "N = len(embedding)\n",
    "#rands = random.rand(N)         # N rand variables sampled from U(0, 1)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000001\n"
     ]
    }
   ],
   "source": [
    "v = embedding[0]\n",
    "sim = cosine_similarity(v, clusterVec[0])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(N):\n",
    "    maxSim = -float('inf')\n",
    "    maxIdx = 0\n",
    "    v = embedding[i]\n",
    "    for j in range(ncluster):\n",
    "        sim = cosine_similarity(v, clusterVec[j])\n",
    "        if sim > maxSim:\n",
    "            maxIdx = j\n",
    "            maxSim = sim\n",
    "    if maxSim < pnew:\n",
    "        if random.random() < pnew:\n",
    "            clusterVec.append(v)\n",
    "            clusterIdx.append([i])\n",
    "            ncluster += 1\n",
    "            pnew = 1.0 / (1 + ncluster)\n",
    "            continue\n",
    "    clusterVec[maxIdx] = clusterVec[maxIdx] +v\n",
    "    clusterIdx[maxIdx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(clusterIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n",
      "1018\n",
      "999\n",
      "3585\n",
      "1133\n",
      "1042\n",
      "937\n",
      "739\n",
      "840\n",
      "811\n",
      "805\n",
      "738\n",
      "909\n",
      "1035\n",
      "901\n",
      "737\n",
      "852\n",
      "763\n",
      "775\n",
      "763\n",
      "704\n",
      "817\n",
      "597\n",
      "476\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(clusterIdx)):\n",
    "    print(len(clusterIdx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easier\n",
      "jalapenos\n",
      "lasagna\n",
      "crusty\n",
      "cubed\n",
      "me\n",
      "care\n",
      "instant\n",
      "bright\n",
      "boil.Stir\n",
      "occasionally.Remove\n",
      "splash\n",
      "dipped\n",
      "type\n",
      "silicone\n",
      "cover.Simmer\n",
      "TO\n",
      "vanilla.Stir\n",
      "425F\n",
      "degrees.Line\n",
      "dusted\n",
      "unsalted\n",
      "dollops\n",
      "peppermint\n",
      "am\n",
      "penne\n",
      "giblets\n",
      "amber\n",
      "With\n",
      "blended.Spoon\n",
      "well.Refrigerate\n",
      "hours.If\n",
      "seven\n",
      "proceed\n",
      "took\n",
      "easiest\n",
      "containing\n",
      "pepper.Divide\n",
      "filo\n",
      "minute.Serve\n",
      "Hot\n",
      "ancho\n",
      "sugar.Cook\n",
      "saving\n",
      "crust.Top\n",
      "thoroughly.Pour\n",
      "overnight.Drain\n",
      "pan.Brush\n",
      "**\n",
      "spray.Sprinkle\n",
      "farro\n",
      "behind\n",
      "content\n",
      "decoratively\n",
      "cant\n",
      "OF\n",
      "browned.Transfer\n",
      "spray.Spread\n",
      "chef\n",
      "minutes.Keep\n",
      "slightly.Remove\n",
      "warm.Combine\n",
      "attach\n",
      "row\n",
      "rinds\n",
      "Grill\n",
      "pepper.Reduce\n",
      "minutes.Immediately\n",
      "towels.In\n",
      "everyone\n",
      "mounding\n",
      "usual\n",
      "grainy\n",
      "hours.For\n",
      "chow\n",
      "up.Add\n",
      "smooth.Preheat\n",
      "plantain\n",
      "blend.Pour\n",
      "water.Boil\n",
      "350F.Place\n",
      "warm.To\n",
      "rice.Cook\n",
      "MEDIUM\n",
      "constantly.Boil\n",
      "burn.Add\n",
      "flour.Place\n",
      "sugar.Set\n",
      "nutmeg.Add\n",
      "W\n",
      "Adjust\n",
      "min.Sprinkle\n",
      "pan.Sift\n",
      "350F.Beat\n",
      "stronger\n",
      "cute\n",
      "chocolate.Place\n",
      "saturated\n",
      "Spinach\n",
      "grill.In\n",
      "alternative\n",
      "ok\n",
      "golden.In\n",
      "organic\n",
      "mixture.When\n",
      "arent\n",
      "bubbly.Stir\n",
      "harina\n",
      "400F.In\n",
      "tripled\n",
      "afraid\n",
      "serving.Add\n",
      "minutes.Chop\n",
      "hamburgers\n",
      "smooth.You\n",
      "crust.Spread\n",
      "extract.Mix\n",
      "smooth.Blend\n",
      "excess.In\n",
      "matcha\n",
      "two-piece\n",
      "middles\n",
      "half.Stir\n",
      "moistened.Spoon\n",
      "burns\n",
      "provide\n",
      "plates.Sprinkle\n",
      "package.Drain\n",
      "lengthways\n",
      "described\n",
      "sauce.Set\n",
      "Texas\n",
      "it.Pour\n",
      "wine.Add\n",
      "Assemble\n",
      "heel\n",
      "pitcher.Add\n",
      "decoration\n",
      "well.Arrange\n",
      "sugar.Continue\n",
      "patties.Grill\n",
      "lollipop\n",
      "cheese.Toss\n",
      "treats\n",
      "oven.Sprinkle\n",
      "silicon\n",
      "veins\n",
      "crumbs.Mix\n",
      "beans.Cook\n",
      "inches.Bring\n",
      "brown.Set\n",
      "woody\n",
      "roulade\n",
      "side.Set\n",
      "cilantro.In\n",
      "pepper.Chill\n",
      "Continue\n",
      "moon\n",
      "lightly.Serve\n",
      "aburaage\n",
      "emulsified\n",
      "container.To\n",
      "stiff.Add\n",
      "Easy\n",
      "dough.Cut\n",
      "parmesan.Bake\n",
      "shakes\n",
      "completely.Make\n",
      "tomatoe\n",
      "choke\n",
      "together.Remove\n",
      "o\n",
      "Hershey\n",
      "juice.Let\n",
      "breadsticks\n",
      "combine.Sprinkle\n",
      "unfold\n",
      "Shape\n",
      "again.Bake\n",
      "layer.In\n",
      "chicken.Top\n",
      "acidity\n",
      "salt.Saute\n",
      "spices.Stir\n",
      "bowl.Allow\n",
      "oil.Drain\n",
      "meets\n",
      "hors\n",
      "blends\n",
      "serve.Store\n",
      "seasoning.Place\n",
      "nestle\n",
      "stickers\n",
      "proceeding\n",
      "saucepan.In\n",
      "rolls.Place\n",
      "350F.Coat\n",
      "mixture.Fill\n",
      "herbed\n",
      "seasoning.In\n",
      "An\n",
      "Baby\n",
      "true\n",
      "57\n",
      "sorrel\n",
      "Cups\n",
      "warm.If\n",
      "el\n",
      "1015\n",
      "ginger.Cook\n",
      "choosing\n",
      "aside.Coat\n",
      "sweeten\n",
      "Dice\n",
      "overnight.Cook\n",
      "marinade.In\n",
      "cheese.For\n",
      "side.For\n",
      "lightly.Refrigerate\n",
      "mushrooms.Stir\n",
      "thick.Put\n",
      "doughs\n",
      "make-ahead\n",
      "spray.Grill\n",
      "heat.Slowly\n",
      "milk.Spread\n",
      "noodles.Spread\n",
      "dissolved.Cool\n",
      "brown.Cook\n",
      "traditionally\n",
      "out.Bake\n",
      "necessary.Transfer\n",
      "water.Chop\n",
      "coated.In\n",
      "ingredients.Using\n",
      "pasta.Stir\n",
      ".Be\n",
      "using.Pour\n",
      "milk.Set\n",
      "dente.Reserve\n",
      "tortillas.Sprinkle\n",
      "Scoop\n",
      "container.Heat\n",
      "thick.With\n",
      "up.Cover\n",
      "burning.Remove\n",
      "burn.When\n",
      "13X9\n",
      "side.Arrange\n",
      "Recipes\n",
      "completely.Repeat\n",
      "bowl.add\n",
      "hour.Cook\n",
      "minutes.Off\n",
      "taste.Ladle\n",
      "batter.Place\n",
      "vegetables.Bake\n",
      "degrees.Brown\n",
      "hours.Then\n",
      "hour.This\n",
      "crumbs.Set\n",
      "cubes.Pour\n",
      "foil.Combine\n",
      "blister\n",
      "0.5\n",
      "di\n",
      "circle.Fold\n",
      "apart.Flatten\n",
      "shrimp.Place\n",
      "spoon.If\n",
      "Baked\n",
      "hands.Form\n",
      "minutes.Stop\n",
      "use.Cut\n",
      "filling.Press\n",
      "minutes.\n",
      "carrots.Add\n",
      "platter.Heat\n",
      "salt2\n",
      "altitude\n",
      "dry.Combine\n",
      "leaf.Add\n",
      "meat.Cut\n",
      "concerned\n",
      "worchestershire\n",
      "surface.With\n",
      "pieces.If\n",
      "majority\n",
      "leaves.Mix\n",
      "mint.Serve\n",
      "speed.Stir\n",
      "afternoon\n",
      "sheet.Beat\n",
      "sugar.Arrange\n",
      "well.Continue\n",
      "pan.Keep\n",
      "2-1/4\n",
      "crank\n",
      "poor\n",
      "crockpot.Cover\n",
      "lettuce.Top\n",
      "days.Serve\n",
      "dressing.Combine\n",
      "cups.Pour\n",
      "well-coated\n",
      "BUT\n",
      "chicken.Put\n",
      "iceberg\n",
      "combine.Top\n",
      "pepper.While\n",
      "pulverize\n",
      "fine.In\n",
      "Romaine\n",
      "filling.Heat\n",
      "pieces.When\n",
      "blended.Sprinkle\n",
      "asideIn\n",
      "surface.Use\n",
      "tomatoes.Heat\n",
      "liquefy\n",
      "V-8\n",
      "slices.Season\n",
      "literally\n",
      "spoon.Do\n",
      "half.Put\n",
      "overtop\n",
      "salad.Top\n",
      "1/3-inch-thick\n",
      "cream.To\n",
      "cool.You\n",
      "evenly.Let\n",
      "wilt.Add\n",
      "milk.The\n",
      "8x4-inch\n",
      "apart.Using\n",
      "overnight.Slice\n",
      "pans.Mix\n",
      "stirring.Cook\n",
      "walnuts.Spoon\n",
      "creamy.Serve\n",
      "thick.You\n",
      "minute.Cook\n",
      "dough.Remove\n",
      "piling\n",
      "set.Garnish\n",
      ".place\n",
      "paste.Whisk\n",
      "vanilla.Transfer\n",
      "liquid.Cut\n",
      "crayfish\n",
      "board.Make\n",
      "marinade.Refrigerate\n",
      "sugar.Refrigerate\n",
      "opposed\n",
      "toss.Cover\n",
      "plate.Let\n",
      "immediately.Peel\n",
      "basil.Bring\n",
      "brown.Peel\n",
      "1/16-inch\n",
      "edges.Turn\n",
      "celery.Saute\n",
      "aside.Line\n",
      "fruit.Place\n",
      "done.Top\n",
      "thymeCombine\n",
      "P\n",
      "carrots.Stir\n",
      "bowl.Boil\n",
      "together.Gently\n",
      "description\n",
      "cream.Using\n",
      "dough.Use\n",
      "farfel\n",
      "balls.Brown\n",
      "tipping\n",
      "excess.Add\n",
      "10-inch-diameter\n",
      "undissolved\n",
      "clear.Place\n",
      "fritos\n",
      "set.Cook\n",
      "coconut.Place\n",
      "half.Preheat\n",
      "well.Leave\n",
      "weak\n",
      "bag.Put\n",
      "covered.Place\n",
      "Reeses\n",
      "pepper.Boil\n",
      "smooth.While\n",
      "nutella\n",
      "sugar.Repeat\n",
      "effect.Bake\n",
      "dry.Set\n",
      "powder.Cut\n",
      "through.Enjoy\n",
      "Tbls\n",
      "skillet.Transfer\n",
      "fry.Add\n",
      "sides.Top\n",
      "shirataki\n",
      "dissolved.Serve\n",
      "permit\n",
      "brown.About\n",
      "400F.Cook\n",
      "Dream\n",
      "container.Store\n",
      "desired.Make\n",
      "Pancetta\n",
      "tin.Place\n",
      "nutmeg.Place\n",
      "skillet.add\n",
      "removes\n",
      "together.Repeat\n",
      "minutes.Whip\n",
      "longer.Drain\n",
      "noodles.Bake\n",
      "egg-milk\n",
      "consistency.When\n",
      "rectangle.Using\n",
      "bruised\n",
      "unfilled\n",
      "onion.Bring\n",
      "guarantee\n",
      "bananas.Mix\n",
      "side.Melt\n",
      "oven.This\n",
      "boil.Immediately\n",
      "wash.Bake\n",
      "dippers\n",
      "powder.Bake\n",
      "well-mixed\n",
      "warm.Boil\n",
      "creamy.Blend\n",
      "Au\n",
      "Carbs\n",
      "double.Bake\n",
      "fluffing\n",
      "rack.Run\n",
      "dish.Fold\n",
      "well.Peel\n",
      "edge.Roll\n",
      "fork.Cover\n",
      "shapes.Bake\n",
      "salt.Dip\n",
      "crisp.Cool\n",
      "oil.With\n",
      "sticky.Place\n",
      "unlit\n",
      "estimated\n",
      "salt.You\n",
      "egg-yolk\n",
      "Ryan\n",
      "Sandwich\n",
      "immediately.You\n",
      "bacon.Top\n",
      "Parmesan.Heat\n",
      "crumbs.Transfer\n",
      "Stew\n",
      "centered\n",
      "hour.It\n",
      "fingers.Bake\n",
      "dressing.Chill\n",
      "stock.Reduce\n",
      "border.Fold\n",
      "coloured.Add\n",
      "Franks\n",
      "zippered\n",
      "poppers\n",
      "hot.Reduce\n",
      "full.Top\n",
      "Parmesan.Season\n",
      "directions.Transfer\n",
      "salsa.Combine\n",
      "filling.Use\n",
      "425.Place\n",
      "occasionally.Using\n",
      "it.Divide\n",
      "COVER\n",
      "inches.Heat\n",
      "cupped\n",
      "middle.Coat\n",
      "Zesty\n",
      "combine.Fill\n",
      "Servings\n",
      "egg.Season\n",
      "minutes.Crumble\n",
      "kumara\n",
      "Parmesan.Cover\n",
      "cook.Place\n",
      "processor.Place\n",
      "desired.Whisk\n",
      "warm.Let\n",
      "sheet.Freeze\n",
      "gravy.Serve\n",
      "elk\n",
      "longer.Cut\n",
      "over.If\n",
      "parsley.Set\n",
      "headspace\n",
      "refrigerate.Place\n",
      "seconds.Return\n",
      "juice.Drain\n",
      "firmly.Bake\n",
      "odd\n",
      "oregano.Mix\n",
      "zigzag\n",
      "Stick\n",
      "325F.Butter\n",
      "balls.Make\n",
      "herbs.Season\n",
      "20-23\n",
      "dry.Arrange\n",
      "cavatelli\n",
      "turned-off\n",
      "wine.Pour\n",
      "sauce.Sauce\n",
      "hot.Meanwhile\n",
      "crowns\n",
      "towels.Fry\n",
      "dish.This\n",
      ".During\n",
      ".Lower\n",
      "ripple\n",
      "butter.On\n",
      "225F\n",
      "portabella\n",
      "biga\n",
      "SURE\n",
      "rack.Invert\n",
      "up.Use\n",
      "forms.Form\n",
      "rack.Prepare\n",
      "evenly.Season\n",
      "desired.When\n",
      "mix.Shape\n",
      "Bouquet\n",
      "combines\n",
      "kumquat\n",
      "olives.Place\n",
      "search\n",
      "pancakes.Serve\n",
      "icing.For\n",
      "necessary.Refrigerate\n",
      "hours.Keep\n",
      "minute.Top\n",
      "baking.When\n",
      "1/3-1/2\n",
      "fins\n",
      "water.Over\n",
      "mediumlow\n",
      "middle.Add\n",
      "overnight.Turn\n",
      "catching\n",
      "fabric\n",
      "towels.Stir\n",
      "seconds.Preheat\n",
      "noodles.Bring\n",
      "regular-size\n",
      "through.Fill\n",
      "chop.Cook\n",
      "sauce.Drizzle\n",
      "hours.Peel\n",
      "mugs.Add\n",
      "pin.Place\n",
      "loosen.Remove\n",
      "doneness.Meanwhile\n",
      "bananas.Spread\n",
      "www.havefunbaking.com\n",
      "using.Cover\n",
      "lid.If\n",
      "seeds.You\n",
      "features\n",
      "spatula.Let\n",
      "half.Divide\n",
      "using.Combine\n",
      "incorporated.Shape\n",
      "olives.In\n",
      "stuffing.In\n",
      "heat.2.Add\n",
      "375.Cook\n",
      "chocolate.Remove\n",
      "sprouts.Cut\n",
      "latex\n",
      "properties\n",
      "paper.The\n",
      "minis\n",
      ".Assemble\n",
      "caramel.Add\n",
      "juice.Reduce\n",
      "them.This\n",
      "flour.Sprinkle\n",
      "tomato.Cook\n",
      "dishes.Serve\n",
      "greens.Mix\n",
      "coat.Cool\n",
      "plate.You\n",
      "spiciness\n",
      "boil.Using\n",
      "bowl.Grill\n",
      "frequently.Sprinkle\n",
      "gently.Stir\n",
      "mixture.Stuff\n",
      "point.Place\n",
      "butter-oil\n",
      "lengths.In\n",
      "leaves.Set\n",
      "wrap.Press\n",
      "slice.Cover\n",
      "blender.Combine\n",
      "instructions.Spoon\n",
      "rope.Cut\n",
      "lemon.Mix\n",
      "leaking\n",
      "PREHEAT\n",
      "inwards\n",
      "hour.Arrange\n",
      "slices.Enjoy\n",
      "bow-tie\n",
      "skillet.Whisk\n",
      "Baste\n",
      "sausage.Cover\n",
      "excess.Sift\n",
      "zucchini.Pour\n",
      "beater.Add\n",
      "flour.Drop\n",
      "completely.You\n",
      "oregano.Saute\n",
      "yolks.Pour\n",
      "rectangles.Press\n",
      "pork.Serve\n",
      "375F.On\n",
      "nuts.Turn\n",
      "lo\n",
      "can.I\n",
      "glisten\n",
      "cold.Cook\n",
      "occasionally.Use\n",
      "tonkatsu\n",
      "egg/sugar\n",
      "Catalina\n",
      "berries.Refrigerate\n",
      "syrup.Shake\n",
      "chips.Refrigerate\n",
      "krispies\n",
      "waters\n",
      "gherkin\n",
      "minutes.Blanch\n",
      "berries.Add\n",
      "fish.Drizzle\n",
      "salmon.Add\n",
      "melted.Continue\n",
      "bubbling.Cool\n",
      "fish.Preheat\n",
      "dip.In\n",
      "Quarter\n",
      "flour.Transfer\n",
      "frige\n",
      "patiently\n",
      "lid.Steam\n",
      "once.Bring\n",
      "degrees.You\n",
      "serve.Position\n",
      "lighten.Fold\n",
      "430\n",
      "occasionally.Spread\n",
      "scorching.Remove\n",
      "golden.Season\n",
      ".FOR\n",
      "bowls.Season\n",
      "eggwhite\n",
      "tomatoes.Put\n",
      "PUREE\n",
      "extract.Spoon\n",
      "Bouillon\n",
      "edges.Preheat\n",
      "off.Roll\n",
      "Fahrenheit.Combine\n",
      "stream.Process\n",
      "herby\n",
      "simmers.Add\n",
      "tortilla.Cook\n",
      "squares.Bring\n",
      "lunches\n",
      "wrap.Spoon\n",
      "potatoes.Melt\n",
      "possible.The\n",
      "cooking.Transfer\n",
      "noodles.Arrange\n",
      "pasillas\n",
      "feeder\n",
      "113\n",
      "half.Fill\n",
      "thickened.Cover\n",
      "on-off\n",
      "Source\n",
      "down.Press\n",
      "cabbage.Stir\n",
      "reserve.To\n",
      "vinaigrette.Preheat\n",
      "bean.Pour\n",
      "china\n",
      "minutes.Variation\n",
      "miutes\n",
      "hot.Make\n",
      "ago\n",
      "necessary.Arrange\n",
      "manage\n",
      "hot.Transfer\n",
      "cook.To\n",
      "forms.Roll\n",
      "square.Using\n",
      "cinnamon.Dip\n",
      "shrimp.Season\n",
      "container.Enjoy\n",
      "halves.Cover\n",
      "side.Use\n",
      "beer.Add\n",
      "softened.Cook\n",
      "melted.Make\n",
      "vinegar.Simmer\n",
      "fine.Heat\n",
      "oven.Return\n",
      "cooked.Cool\n",
      "tortillas.Divide\n",
      "8-in.square\n",
      "golden.Keep\n",
      "slices.Peel\n",
      "beef.Cut\n",
      "marinade.Combine\n",
      "SHAKE\n",
      "cook.If\n",
      "seeds.Grill\n",
      "Puck\n",
      "fat.Turn\n",
      "w/the\n",
      "evenly.Roll\n",
      "towelling\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(clusterIdx[7])):\n",
    "    print(vocab.id2word[clusterIdx[7][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
