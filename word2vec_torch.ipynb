{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "# 必要であれば変更してください\n",
    "import os\n",
    "os.chdir('/root/userspace/team9_project')\n",
    "'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "坊主\tボウズ\t坊主\t名詞-一般\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "屏風\tビョウブ\t屏風\t名詞-一般\t\t\n",
      "に\tニ\tに\t助詞-格助詞-一般\t\t\n",
      "上手\tジョウズ\t上手\t名詞-形容動詞語幹\t\t\n",
      "に\tニ\tに\t助詞-副詞化\t\t\n",
      "坊主\tボウズ\t坊主\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "絵\tエ\t絵\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "描い\tエガイ\t描く\t動詞-自立\t五段・カ行イ音便\t連用タ接続\n",
      "た\tタ\tた\t助動詞\t特殊・タ\t基本形\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "node = tagger.parse(\"坊主が屏風に上手に坊主の絵を描いた\")\n",
    "print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"日本語の文を形態素の列に分割する関数\n",
    "\n",
    "    :param sentence: str, 日本語の文\n",
    "    :return tokenized_sentence: list of str, 形態素のリスト\n",
    "    \"\"\"\n",
    "    node = tagger.parse(sentence)\n",
    "    node = node.split(\"\\n\")\n",
    "    tokenized_sentence = []\n",
    "    for i in range(len(node)):\n",
    "        feature = node[i].split(\"\\t\")\n",
    "        if feature[0] == \"EOS\":\n",
    "            # 文が終わったら終了\n",
    "            break\n",
    "        # 分割された形態素を追加\n",
    "        tokenized_sentence.append(feature[0])\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"『こころ』を読み込むための関数\n",
    "\n",
    "    :param path: str, 『こころ』のパス\n",
    "    :return text: list of list of str, 各文がトークナイズされた『こころ』\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = tokenize(line)\n",
    "            text.append(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenize(df):\n",
    "    text=[]\n",
    "    for recipe in df:\n",
    "        word = tokenize(recipe)\n",
    "        text.append(word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c10a78f5a994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/Material/recipe1M/recipe1M_0.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#text = load_data(\"dataset/Material/recipes2.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#text = load_data(\"dataset/kokoro.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-eed9462ea119>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cc5e5c7ca011>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "text = load_data(\"dataset/Material/recipe1M/recipe1M_0.txt\")\n",
    "#text = load_data(\"dataset/Material/recipes2.txt\")\n",
    "#text = load_data(\"dataset/kokoro.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ボウル',\n",
       " 'に',\n",
       " '粉',\n",
       " 'と',\n",
       " 'ベーキングパウダー',\n",
       " 'を',\n",
       " '入れ',\n",
       " 'て',\n",
       " '混ぜ合わ',\n",
       " 'せる',\n",
       " '。',\n",
       " '1',\n",
       " 'に',\n",
       " 'オリーブ',\n",
       " 'オイル',\n",
       " 'を',\n",
       " '加え',\n",
       " 'て',\n",
       " '手',\n",
       " 'で',\n",
       " '混ぜる',\n",
       " '。',\n",
       " '水',\n",
       " 'を',\n",
       " 'まず',\n",
       " '半量',\n",
       " '加え',\n",
       " 'て',\n",
       " 'よく',\n",
       " '混ぜ',\n",
       " '、',\n",
       " '残り',\n",
       " 'の',\n",
       " '半量',\n",
       " 'を',\n",
       " '加え',\n",
       " 'たら',\n",
       " '手',\n",
       " 'に',\n",
       " '付か',\n",
       " 'なく',\n",
       " 'なる',\n",
       " 'まで',\n",
       " 'よく',\n",
       " '混ぜる',\n",
       " '。',\n",
       " '全体',\n",
       " 'を',\n",
       " 'まとめ',\n",
       " 'て',\n",
       " '天',\n",
       " '板',\n",
       " 'の',\n",
       " '大き',\n",
       " 'さ',\n",
       " 'に',\n",
       " '綿棒',\n",
       " 'で',\n",
       " 'のばす',\n",
       " '。',\n",
       " 'クッキング',\n",
       " 'シート',\n",
       " 'の',\n",
       " '上',\n",
       " 'に',\n",
       " '生地',\n",
       " 'を',\n",
       " 'のせ',\n",
       " 'ピザ',\n",
       " 'ソース',\n",
       " 'の',\n",
       " '材料',\n",
       " 'を',\n",
       " '混ぜ合わ',\n",
       " 'せ',\n",
       " '伸ばし',\n",
       " 'た',\n",
       " '生地',\n",
       " 'に',\n",
       " '塗る',\n",
       " '。',\n",
       " '好み',\n",
       " 'の',\n",
       " '具',\n",
       " '材',\n",
       " 'を',\n",
       " 'のせる',\n",
       " '。',\n",
       " 'オリーブ',\n",
       " 'オイル',\n",
       " '(',\n",
       " '分量',\n",
       " '外',\n",
       " ')',\n",
       " 'を',\n",
       " 'かけ',\n",
       " '、',\n",
       " '余熱',\n",
       " 'あり',\n",
       " '250',\n",
       " '℃',\n",
       " '以上',\n",
       " '(',\n",
       " '我が家',\n",
       " 'は',\n",
       " '300',\n",
       " '℃)',\n",
       " 'で',\n",
       " '10',\n",
       " '〜',\n",
       " '15',\n",
       " '分',\n",
       " '焼い',\n",
       " 'て',\n",
       " '出来上がり',\n",
       " '♪',\n",
       " '生地',\n",
       " 'に',\n",
       " 'いり',\n",
       " 'ゴマ',\n",
       " 'を',\n",
       " '混ぜ',\n",
       " '、',\n",
       " 'オリーブ',\n",
       " 'オイル',\n",
       " 'を',\n",
       " 'ゴマ油',\n",
       " 'に',\n",
       " '変更',\n",
       " '♪',\n",
       " 'ドライ',\n",
       " 'カレー',\n",
       " 'を',\n",
       " 'のっけ',\n",
       " 'て',\n",
       " 'ウマウマ',\n",
       " '(^¬^)♪']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊なトークンとそのIDは事前に定義しておきます。\n",
    "PAD_TOKEN = '<PAD>' # あとで説明するpaddingに使います\n",
    "UNK_TOKEN = '<UNK>' # 辞書にない単語は全てこのUNKトークンに置き換えます。(UNKの由来はunkownです)\n",
    "PAD = 0 # <PAD>のID\n",
    "UNK = 1 # <UNK>のID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書の初期化\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "# 辞書に含める単語の最低出現回数\n",
    "# 今回はコーパスのサイズが小さいので、全ての単語を辞書に含めることにします\n",
    "MIN_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"語彙を管理するためのクラス\"\"\"\n",
    "    def __init__(self, word2id={}):\n",
    "        \"\"\"\n",
    "        :param word2id: 単語(str)をインデックス(int)に変換する辞書\n",
    "        \"\"\"\n",
    "        self.word2id = dict(word2id)\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
    "\n",
    "    def build_vocab(self, sentences, min_count=1):\n",
    "        \"\"\"コーパスから語彙の辞書を構築するメソッド\n",
    "\n",
    "        :param sentences: list of list of str, コーパス\n",
    "        :param min_count: int, 辞書に含める単語の最小出現回数\n",
    "        \"\"\"\n",
    "        # 各単語の出現回数をカウントする辞書を作成します\n",
    "        word_counter = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                # dict.get(key, 0)はdictにkeyがあればdict[key]を、なければ0を返すメソッドです\n",
    "                word_counter[word] = word_counter.get(word, 0) + 1\n",
    "\n",
    "        # min_count回以上出現する単語のみ語彙に加えます\n",
    "        # 出現回数の高い単語から順にword2idに追加していきます\n",
    "        # 出現回数に-1をかけた値でsortすることで出現回数の降順になるようにしています\n",
    "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
    "            if count < min_count:\n",
    "                break\n",
    "            _id = len(self.word2id)\n",
    "            self.word2id.setdefault(word, _id)\n",
    "            self.id2word[_id] = word\n",
    "\n",
    "        # 語彙に含まれる単語の出現回数を保持します（あとで使います）\n",
    "        self.raw_vocab = {w: word_counter[w] for w in self.word2id.keys() if w in word_counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙数: 7923\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(text, min_count=MIN_COUNT)\n",
    "print(\"語彙数:\", len(vocab.word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sen):\n",
    "    \"\"\"\n",
    "    単語のリストをIDのリストに変換する関数\n",
    "\n",
    "    :param vocab: class `Vocab` object\n",
    "    :param sen: list of str, 文を分かち書きして得られた単語のリスト\n",
    "    :return out: list of int, 単語IDのリスト\n",
    "    \"\"\"\n",
    "    out = [vocab.word2id.get(word, UNK) for word in sen] # 辞書にない単語にはUNKのIDを割り振ります\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語のテキストを単語IDに変換します。\n",
    "id_text = [sentence_to_ids(vocab, sen) for sen in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['こころ']\n",
      "[4150]\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(id_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    \"\"\"Paddingを行う関数\n",
    "\n",
    "    :param seq: list of int, 単語のインデックスのリスト\n",
    "    :param max_length: int, バッチ内の系列の最大長\n",
    "    :return seq: list of int, 単語のインデックスのリスト\n",
    "    \"\"\"\n",
    "    seq += [PAD for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 64 # ミニバッチのサイズ\n",
    "n_batches = 900 # 今回学習するミニバッチの数\n",
    "vocab_size = len(vocab.word2id) # 語彙の総数\n",
    "embedding_size = 600 # 各単語に割り当てるベクトルの次元数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestIter(object):\n",
    "    def __init__(self):\n",
    "        self.iter = 0\n",
    "        self.max_iter = 5\n",
    "    \n",
    "    def __iter__(self): # 必須\n",
    "        print(\"iter関数が呼び出されました\")\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.iter += 1\n",
    "        print(\"next関数が呼び出されました({}回目)\".format(self.iter))\n",
    "        if self.iter < self.max_iter:\n",
    "            return None\n",
    "        else:\n",
    "            print(\"max_iterに達したので終了します\")\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderCBOW(object):\n",
    "    \"\"\"CBOWのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語とターゲットの単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            # 走査する対象の文\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # 予測すべき単語\n",
    "            word_Y = sen[self.w_pointer]\n",
    "            \n",
    "            # 入力となる単語群を取得\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_X = pad_seq(word_X, self.window * 2)\n",
    "            \n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "            \n",
    "            if self.w_pointer >= len(sen):\n",
    "                # 文を走査し終わったら次の文の先頭にポインタを移行する\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    # 全ての文を走査し終わったら終了する\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        # データはtorch.Tensorにする必要があります。dtype, deviceも指定します。\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 全結合層(バイアスなし)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), CBOWのloss\n",
    "        \"\"\"\n",
    "        emb_X = self.embedding(batch_X) # (batch_size, window*2, embedding_size)\n",
    "        # paddingした部分を無視するためにマスクをかけます\n",
    "        emb_X = emb_X * (batch_X != PAD).float().unsqueeze(-1) # (batch_size, window*2, embedding_size)\n",
    "        sum_X = torch.sum(emb_X, dim=1) # (batch_size, embedding_size)\n",
    "        lin_X = self.linear(sum_X) # (batch_size, vocab_size)\n",
    "        log_prob_X = F.log_softmax(lin_X, dim=-1) # (batch_size, vocab_size)\n",
    "        loss = F.nll_loss(log_prob_X, batch_Y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル\n",
    "cbow = CBOW(vocab_size, embedding_size).to(device) # iLectで実行する場合warning (GPU is too old) が出ますが, 実行に問題はないので気にせず進めてください.\n",
    "# optimizer\n",
    "optimizer_cbow = optim.Adam(cbow.parameters())\n",
    "# データローダー\n",
    "dataloader_cbow = DataLoaderCBOW(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, inputs, optimizer=None, is_train=True):\n",
    "    \"\"\"lossを計算するための関数\n",
    "    \n",
    "    is_train=Trueならモデルをtrainモードに、\n",
    "    is_train=Falseならモデルをevaluationモードに設定します\n",
    "    \n",
    "    :param model: 学習させるモデル\n",
    "    :param inputs: モデルへの入力\n",
    "    :param optimizer: optimizer\n",
    "    :param is_train: bool, モデルtrainさせるか否か\n",
    "    \"\"\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    # lossを計算します。\n",
    "    loss = model(*inputs)\n",
    "\n",
    "    if is_train:\n",
    "        # .backward()を実行する前にmodelのparameterのgradientを全て0にセットします\n",
    "        optimizer.zero_grad()\n",
    "        # parameterのgradientを計算します。\n",
    "        loss.backward()\n",
    "        # parameterのgradientを用いてparameterを更新します。\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:8.9773\n",
      "batch:100, loss:5.8359\n",
      "batch:200, loss:5.2109\n",
      "batch:300, loss:6.9613\n",
      "batch:400, loss:5.2537\n",
      "batch:500, loss:5.4618\n",
      "batch:600, loss:6.5011\n",
      "batch:700, loss:5.0042\n",
      "batch:800, loss:5.5086\n",
      "batch:900, loss:5.8533\n",
      "Elapsed time: 7.53 [sec]\n"
     ]
    }
   ],
   "source": [
    "start_at = time.time()\n",
    "\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_cbow):\n",
    "    loss = compute_loss(cbow, (batch_X, batch_Y), optimizer=optimizer_cbow, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "\n",
    "end_at = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"kokoro_cbow_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(cbow.embedding.weight.data.cpu().numpy(),  \"recipe_cbow_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSG(object):\n",
    "    \"\"\"Skipgramのためのデータローダー\"\"\"\n",
    "    def __init__(self, text, batch_size, window=3):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.s_pointer = 0 # データセット上を走査する文単位のポインタ\n",
    "        self.w_pointer = 0 # データセット上を走査する単語単位のポインタ\n",
    "        self.max_s_pointer = len(text) # データセットに含まれる文の総数\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            \n",
    "            # Skipgramでは入力が1単語\n",
    "            word_X = sen[self.w_pointer]\n",
    "\n",
    "            # 出力は周辺単語\n",
    "            start = max(0,self.w_pointer-self.window)\n",
    "            word_Y = sen[start:self.w_pointer]+sen[self.w_pointer+1:self.w_pointer+self.window+1]\n",
    "            word_Y = pad_seq(word_Y,self.window*2)\n",
    "\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "            self.w_pointer += 1\n",
    "\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch_X, batch_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.linear = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y):\n",
    "        \"\"\"\n",
    "        :param batch_X: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_Y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :return loss: torch.Tensor(dtype=torch.float), Skipgramのloss\n",
    "        \"\"\"\n",
    "        emb_X = self.embedding(batch_X)\n",
    "        lin_X = self.linear(emb_X)\n",
    "        log_prob_X = F.log_softmax(lin_X,dim=-1)\n",
    "        log_prob_X = torch.gather(log_prob_X, 1, batch_Y) # (batch_size, window*2)\n",
    "        # paddingした単語のlossは計算しないようにマスクをかけます(=lossの該当部分を0にします)\n",
    "        log_prob_X = log_prob_X * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss = log_prob_X.sum(1).mean().neg()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Skipgram(vocab_size, embedding_size).to(device)\n",
    "optimizer_sg = optim.Adam(sg.parameters())\n",
    "dataloader_sg = DataLoaderSG(id_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:48.2575\n",
      "batch:100, loss:38.6715\n",
      "batch:200, loss:34.5417\n",
      "batch:300, loss:45.8497\n",
      "batch:400, loss:36.2331\n",
      "batch:500, loss:32.9121\n",
      "batch:600, loss:45.5428\n",
      "batch:700, loss:33.5448\n",
      "batch:800, loss:37.3178\n",
      "batch:900, loss:42.0639\n",
      "Elapsed time: 7.37 [sec]\n"
     ]
    }
   ],
   "source": [
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y) in enumerate(dataloader_sg):\n",
    "    loss = compute_loss(sg, (batch_X, batch_Y), optimizer=optimizer_sg, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み層のパラメータのみを保存する\n",
    "torch.save(sg.embedding.weight.data.cpu().numpy(),  \"kokoro_sg_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(sg.embedding.weight.data.cpu().numpy(),  \"recipe_sg_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skipgram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samplingに使う確率分布\n",
    "weights = np.power([0, 0] + list(vocab.raw_vocab.values()), 0.75)\n",
    "weights = weights / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderSGNS(object):\n",
    "    def __init__(self, text, batch_size, window=3, n_negative=5, weights=None):\n",
    "        \"\"\"\n",
    "        :param text: list of list of int, 単語をIDに変換したデータセット\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param window: int, 周辺単語と入力単語の最大距離\n",
    "        :param n_negative: int, 負例の数\n",
    "        :param weights: numpy.ndarray, Negative Samplingで使う確率分布\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.n_negative = n_negative\n",
    "        self.weights = None\n",
    "        if weights is not None:\n",
    "            self.weights = torch.FloatTensor(weights) # negative samplingに使う確率分布\n",
    "        self.s_pointer = 0 # 文のポインタ\n",
    "        self.w_pointer = 0 # 単語のポインタ\n",
    "        self.max_s_pointer = len(text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_X = []\n",
    "        batch_Y = []\n",
    "        batch_N = [] # 負例\n",
    "        while len(batch_X) < self.batch_size:\n",
    "            sen = self.text[self.s_pointer]\n",
    "            start = max(0, self.w_pointer - self.window)\n",
    "            word_X = sen[self.w_pointer]\n",
    "            word_Y = sen[start:self.w_pointer] + \\\n",
    "                sen[self.w_pointer + 1:self.w_pointer + self.window + 1]\n",
    "            word_Y = pad_seq(word_Y, self.window * 2)\n",
    "            batch_X.append(word_X)\n",
    "            batch_Y.append(word_Y)\n",
    "\n",
    "            # 多項分布で負例をサンプリング\n",
    "            # 実装を簡略化するために、正例の除去は行っていません\n",
    "            negative_samples = torch.multinomial(self.weights, self.n_negative) # (n_negative,)\n",
    "            batch_N.append(negative_samples.unsqueeze(0)) # (1, n_negative)\n",
    "\n",
    "            self.w_pointer += 1\n",
    "            if self.w_pointer >= len(sen):\n",
    "                self.w_pointer = 0\n",
    "                self.s_pointer += 1\n",
    "                if self.s_pointer >= self.max_s_pointer:\n",
    "                    self.s_pointer = 0\n",
    "                    raise StopIteration\n",
    "\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device)\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device)\n",
    "        batch_N = torch.cat(batch_N, dim=0).to(device) # (batch_size, n_negative)\n",
    "\n",
    "        return batch_X, batch_Y, batch_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param embedding_size: int, 単語埋め込みベクトルの次元\n",
    "        \"\"\"\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 入力単語の埋め込み層\n",
    "        self.i_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # 出力単語の埋め込み層\n",
    "        self.o_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.i_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.o_embedding.weight)\n",
    "\n",
    "    def forward(self, batch_X, batch_Y, batch_N):\n",
    "        \"\"\"\n",
    "        :param batch_x: torch.Tensor(dtype=torch.long), (batch_size,)\n",
    "        :param batch_y: torch.Tensor(dtype=torch.long), (batch_size, window*2)\n",
    "        :param batch_n: torch.Tensor(dtype=torch.long), (batch_size, n_negative)\n",
    "        \"\"\"\n",
    "        embed_X = self.i_embedding(batch_X).unsqueeze(2) # (batch_size, embedding_size, 1)\n",
    "        embed_Y = self.o_embedding(batch_Y) # (batch_size, window*2, embedding_size)\n",
    "        embed_N = self.o_embedding(batch_N).neg() # (batch_size, n_negative, embedding_size)\n",
    "        loss_Y = torch.bmm(embed_Y, embed_X).squeeze().sigmoid().log() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y * (batch_Y != PAD).float() # (batch_size, window*2)\n",
    "        loss_Y = loss_Y.sum(1) # (batch_size,)\n",
    "        loss_N = torch.bmm(embed_N, embed_X).squeeze().sigmoid().log().sum(1) # (batch_size,)\n",
    "        return -(loss_Y + loss_N).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, loss:7.0823\n",
      "batch:100, loss:7.0294\n",
      "batch:200, loss:6.5883\n",
      "batch:300, loss:7.3731\n",
      "batch:400, loss:6.7110\n",
      "batch:500, loss:6.6285\n",
      "batch:600, loss:7.2633\n",
      "batch:700, loss:6.3686\n",
      "batch:800, loss:6.7104\n",
      "batch:900, loss:7.1610\n",
      "Elapsed time: 40.13 [sec]\n"
     ]
    }
   ],
   "source": [
    "sgns = SGNS(vocab_size, embedding_size).to(device)\n",
    "optimizer_sgns = optim.Adam(sgns.parameters())\n",
    "dataloader_sgns = DataLoaderSGNS(id_text, batch_size, n_negative=5, weights=weights)\n",
    "start_at = time.time()\n",
    "for batch_id, (batch_X, batch_Y, batch_N) in enumerate(dataloader_sgns):\n",
    "    loss = compute_loss(sgns, (batch_X, batch_Y, batch_N), optimizer=optimizer_sgns, is_train=True)\n",
    "    if batch_id % 100 == 0:\n",
    "        print(\"batch:{}, loss:{:.4f}\".format(batch_id, loss))\n",
    "    if batch_id >= n_batches:\n",
    "        break\n",
    "end_at = time.time()\n",
    "print(\"Elapsed time: {:.2f} [sec]\".format(end_at - start_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddingのパラメータのみを保存する\n",
    "torch.save(sgns.i_embedding.weight.data.cpu().numpy(),  \"kokoro_sgns_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))\n",
    "#torch.save(sgns.i_embedding.weight.data.cpu().numpy(),  \"recipe_sgns_embedding_{}b_min_{}_dim_{}.pth\".format(n_batches,MIN_COUNT,embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_similarity(embedding_path, word, n):\n",
    "    \"\"\"\n",
    "    与えられた単語に最も似ている単語とcos類似度を返す関数\n",
    "\n",
    "    :param embedding_path: str, 保存した埋め込み層のパラメータのパス\n",
    "    :param word: str, 単語\n",
    "    :param n: int\n",
    "    :return out: str, 上位n個の類似単語とそのcos類似度\n",
    "    \"\"\"\n",
    "    embedding = torch.load(embedding_path)\n",
    "\n",
    "    # 単語ベクトルを全て単位ベクトルにする\n",
    "    norm = np.linalg.norm(embedding, ord=2, axis=1, keepdims=True)\n",
    "    norm = np.where(norm==0, 1, norm) # 0で割ることを避ける\n",
    "    embedding /= norm\n",
    "    e = embedding[vocab.word2id[word]]\n",
    "\n",
    "    # 単語ベクトル同士のcos類似度を計算する\n",
    "    cos_sim = np.dot(embedding, e.reshape(-1, 1)).reshape(-1,)\n",
    "    most_sim = np.argsort(cos_sim)[::-1][1:n+1] # 自分は除く\n",
    "    most_sim_words = [vocab.id2word[_id] for _id in most_sim]\n",
    "    top_cos_sim = cos_sim[most_sim]\n",
    "    out = \", \".join([w+\"({:.4f})\".format(v) for w, v in zip(most_sim_words, top_cos_sim)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow\t: 父(0.6770), 奥さん(0.6755), 彼ら(0.6735), 医者(0.6470), あなた(0.6346)\n",
      "sg\t: 形(0.1762), ずらし(0.1708), 少ない(0.1474), 弱く(0.1407), 動き出し(0.1378)\n",
      "sgns\t: あなた(0.6294), はたして(0.5896), 先生(0.5766), しかし(0.5613), 奥さん(0.5569)\n"
     ]
    }
   ],
   "source": [
    "# kokoro\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"kokoro_\"+model + \"_embedding_900b_min_1_dim_600.pth\", \"私\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kokoro_cbow_embedding_300b_min_3.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-badb8d6d7bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     print(model+\"\\t:\", compute_word_similarity(\n\u001b[0;32m----> 5\u001b[0;31m         \"kokoro_\"+model + \"_embedding_300b_min_3.pth\", \"私\", 5))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-182-e9f564237daa>\u001b[0m in \u001b[0;36mcompute_word_similarity\u001b[0;34m(embedding_path, word, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m上位n個の類似単語とそのcos類似度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 単語ベクトルを全て単位ベクトルにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kokoro_cbow_embedding_300b_min_3.pth'"
     ]
    }
   ],
   "source": [
    "# 300バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"recipe_\"+model + \"_embedding_300b_min_3.pth\", \"牛肉\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow\t: ひとつまみ(0.8166), 青(0.8139), さめ(0.8124), 残し(0.8072), 流し込む(0.8056)\n",
      "sg\t: 煮え(0.1487), やつ(0.1441), 振り掛ける(0.1436), マグネシウム(0.1391), 鳴っ(0.1371)\n",
      "sgns\t: おい(0.9195), みじん切り(0.9177), 目(0.9169), 袋(0.9164), 酢(0.9164)\n"
     ]
    }
   ],
   "source": [
    "# 900バッチだけ学習した時\n",
    "models = [\"cbow\", \"sg\", \"sgns\"]\n",
    "for model in models:\n",
    "    print(model+\"\\t:\", compute_word_similarity(\n",
    "        \"recipe_\"+ model + \"_embedding_900b_min_2_dim_600.pth\", \"牛肉\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
